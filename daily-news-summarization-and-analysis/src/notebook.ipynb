{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 -qq install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Program Files\\Python312\\Lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Program Files\\Python312\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from datetime import datetime, timedelta\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from fuzzywuzzy import fuzz\n",
    "import spacy\n",
    "import keras\n",
    "from keybert import KeyBERT\n",
    "from langdetect import detect\n",
    "import textwrap\n",
    "import faiss\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import openai\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema import HumanMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;3m⚠ As of spaCy v3.0, shortcuts like 'en' are deprecated. Please use the\n",
      "full pipeline package name 'en_core_web_sm' instead.\u001b[0m\n",
      "Collecting en-core-web-sm==3.7.1\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
      "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "     --------------------------------------- 0.0/12.8 MB 131.3 kB/s eta 0:01:38\n",
      "     --------------------------------------- 0.0/12.8 MB 163.8 kB/s eta 0:01:18\n",
      "     --------------------------------------- 0.0/12.8 MB 195.7 kB/s eta 0:01:06\n",
      "     --------------------------------------- 0.1/12.8 MB 374.1 kB/s eta 0:00:34\n",
      "      -------------------------------------- 0.2/12.8 MB 615.9 kB/s eta 0:00:21\n",
      "      -------------------------------------- 0.2/12.8 MB 758.5 kB/s eta 0:00:17\n",
      "     - ------------------------------------- 0.4/12.8 MB 967.1 kB/s eta 0:00:13\n",
      "     - -------------------------------------- 0.4/12.8 MB 1.1 MB/s eta 0:00:12\n",
      "     - -------------------------------------- 0.6/12.8 MB 1.2 MB/s eta 0:00:11\n",
      "     -- ------------------------------------- 0.7/12.8 MB 1.4 MB/s eta 0:00:09\n",
      "     -- ------------------------------------- 0.7/12.8 MB 1.4 MB/s eta 0:00:09\n",
      "     -- ------------------------------------- 0.8/12.8 MB 1.5 MB/s eta 0:00:09\n",
      "     --- ------------------------------------ 1.0/12.8 MB 1.6 MB/s eta 0:00:08\n",
      "     --- ------------------------------------ 1.1/12.8 MB 1.6 MB/s eta 0:00:08\n",
      "     --- ------------------------------------ 1.2/12.8 MB 1.7 MB/s eta 0:00:07\n",
      "     ---- ----------------------------------- 1.3/12.8 MB 1.7 MB/s eta 0:00:07\n",
      "     ---- ----------------------------------- 1.5/12.8 MB 1.8 MB/s eta 0:00:07\n",
      "     ----- ---------------------------------- 1.7/12.8 MB 1.9 MB/s eta 0:00:06\n",
      "     ----- ---------------------------------- 1.8/12.8 MB 1.9 MB/s eta 0:00:06\n",
      "     ----- ---------------------------------- 1.8/12.8 MB 1.9 MB/s eta 0:00:06\n",
      "     ----- ---------------------------------- 1.8/12.8 MB 1.8 MB/s eta 0:00:07\n",
      "     ------ --------------------------------- 2.1/12.8 MB 2.0 MB/s eta 0:00:06\n",
      "     ------ --------------------------------- 2.2/12.8 MB 1.9 MB/s eta 0:00:06\n",
      "     ------- -------------------------------- 2.3/12.8 MB 1.9 MB/s eta 0:00:06\n",
      "     ------- -------------------------------- 2.4/12.8 MB 2.0 MB/s eta 0:00:06\n",
      "     ------- -------------------------------- 2.5/12.8 MB 2.0 MB/s eta 0:00:06\n",
      "     -------- ------------------------------- 2.6/12.8 MB 2.0 MB/s eta 0:00:06\n",
      "     -------- ------------------------------- 2.7/12.8 MB 2.0 MB/s eta 0:00:06\n",
      "     -------- ------------------------------- 2.8/12.8 MB 2.0 MB/s eta 0:00:06\n",
      "     --------- ------------------------------ 2.9/12.8 MB 2.0 MB/s eta 0:00:05\n",
      "     --------- ------------------------------ 3.1/12.8 MB 2.0 MB/s eta 0:00:05\n",
      "     --------- ------------------------------ 3.1/12.8 MB 2.0 MB/s eta 0:00:05\n",
      "     --------- ------------------------------ 3.1/12.8 MB 2.0 MB/s eta 0:00:05\n",
      "     ---------- ----------------------------- 3.4/12.8 MB 2.0 MB/s eta 0:00:05\n",
      "     ---------- ----------------------------- 3.5/12.8 MB 2.1 MB/s eta 0:00:05\n",
      "     ----------- ---------------------------- 3.5/12.8 MB 2.1 MB/s eta 0:00:05\n",
      "     ----------- ---------------------------- 3.6/12.8 MB 2.0 MB/s eta 0:00:05\n",
      "     ----------- ---------------------------- 3.6/12.8 MB 2.0 MB/s eta 0:00:05\n",
      "     ------------ --------------------------- 3.9/12.8 MB 2.1 MB/s eta 0:00:05\n",
      "     ------------ --------------------------- 4.1/12.8 MB 2.1 MB/s eta 0:00:05\n",
      "     ------------- -------------------------- 4.2/12.8 MB 2.1 MB/s eta 0:00:05\n",
      "     ------------- -------------------------- 4.3/12.8 MB 2.1 MB/s eta 0:00:05\n",
      "     ------------- -------------------------- 4.4/12.8 MB 2.1 MB/s eta 0:00:04\n",
      "     ------------- -------------------------- 4.5/12.8 MB 2.1 MB/s eta 0:00:04\n",
      "     -------------- ------------------------- 4.6/12.8 MB 2.1 MB/s eta 0:00:04\n",
      "     -------------- ------------------------- 4.7/12.8 MB 2.1 MB/s eta 0:00:04\n",
      "     -------------- ------------------------- 4.8/12.8 MB 2.1 MB/s eta 0:00:04\n",
      "     --------------- ------------------------ 4.8/12.8 MB 2.1 MB/s eta 0:00:04\n",
      "     --------------- ------------------------ 5.0/12.8 MB 2.1 MB/s eta 0:00:04\n",
      "     --------------- ------------------------ 5.1/12.8 MB 2.1 MB/s eta 0:00:04\n",
      "     ---------------- ----------------------- 5.2/12.8 MB 2.1 MB/s eta 0:00:04\n",
      "     ---------------- ----------------------- 5.2/12.8 MB 2.1 MB/s eta 0:00:04\n",
      "     ---------------- ----------------------- 5.3/12.8 MB 2.1 MB/s eta 0:00:04\n",
      "     ----------------- ---------------------- 5.5/12.8 MB 2.1 MB/s eta 0:00:04\n",
      "     ----------------- ---------------------- 5.6/12.8 MB 2.1 MB/s eta 0:00:04\n",
      "     ------------------ --------------------- 5.8/12.8 MB 2.1 MB/s eta 0:00:04\n",
      "     ------------------ --------------------- 5.9/12.8 MB 2.1 MB/s eta 0:00:04\n",
      "     ------------------ --------------------- 6.0/12.8 MB 2.1 MB/s eta 0:00:04\n",
      "     ------------------ --------------------- 6.1/12.8 MB 2.1 MB/s eta 0:00:04\n",
      "     ------------------ --------------------- 6.1/12.8 MB 2.1 MB/s eta 0:00:04\n",
      "     ------------------- -------------------- 6.1/12.8 MB 2.1 MB/s eta 0:00:04\n",
      "     ------------------- -------------------- 6.2/12.8 MB 2.1 MB/s eta 0:00:04\n",
      "     ------------------- -------------------- 6.3/12.8 MB 2.1 MB/s eta 0:00:04\n",
      "     ------------------- -------------------- 6.3/12.8 MB 2.1 MB/s eta 0:00:04\n",
      "     ------------------- -------------------- 6.4/12.8 MB 2.1 MB/s eta 0:00:04\n",
      "     -------------------- ------------------- 6.4/12.8 MB 2.0 MB/s eta 0:00:04\n",
      "     -------------------- ------------------- 6.5/12.8 MB 2.0 MB/s eta 0:00:04\n",
      "     -------------------- ------------------- 6.6/12.8 MB 2.0 MB/s eta 0:00:04\n",
      "     -------------------- ------------------- 6.6/12.8 MB 2.0 MB/s eta 0:00:04\n",
      "     -------------------- ------------------- 6.6/12.8 MB 2.0 MB/s eta 0:00:04\n",
      "     -------------------- ------------------- 6.6/12.8 MB 2.0 MB/s eta 0:00:04\n",
      "     -------------------- ------------------- 6.6/12.8 MB 2.0 MB/s eta 0:00:04\n",
      "     --------------------- ------------------ 6.9/12.8 MB 2.0 MB/s eta 0:00:03\n",
      "     ---------------------- ----------------- 7.1/12.8 MB 2.0 MB/s eta 0:00:03\n",
      "     ---------------------- ----------------- 7.2/12.8 MB 2.0 MB/s eta 0:00:03\n",
      "     ---------------------- ----------------- 7.2/12.8 MB 2.0 MB/s eta 0:00:03\n",
      "     ---------------------- ----------------- 7.4/12.8 MB 2.0 MB/s eta 0:00:03\n",
      "     ----------------------- ---------------- 7.4/12.8 MB 2.0 MB/s eta 0:00:03\n",
      "     ----------------------- ---------------- 7.5/12.8 MB 2.0 MB/s eta 0:00:03\n",
      "     ----------------------- ---------------- 7.6/12.8 MB 2.0 MB/s eta 0:00:03\n",
      "     ------------------------ --------------- 7.7/12.8 MB 2.0 MB/s eta 0:00:03\n",
      "     ------------------------ --------------- 7.8/12.8 MB 2.0 MB/s eta 0:00:03\n",
      "     ------------------------ --------------- 7.9/12.8 MB 2.0 MB/s eta 0:00:03\n",
      "     ------------------------ --------------- 7.9/12.8 MB 2.0 MB/s eta 0:00:03\n",
      "     ------------------------- -------------- 8.0/12.8 MB 2.0 MB/s eta 0:00:03\n",
      "     ------------------------- -------------- 8.1/12.8 MB 1.9 MB/s eta 0:00:03\n",
      "     ------------------------- -------------- 8.2/12.8 MB 2.0 MB/s eta 0:00:03\n",
      "     -------------------------- ------------- 8.3/12.8 MB 2.0 MB/s eta 0:00:03\n",
      "     -------------------------- ------------- 8.3/12.8 MB 2.0 MB/s eta 0:00:03\n",
      "     -------------------------- ------------- 8.4/12.8 MB 1.9 MB/s eta 0:00:03\n",
      "     -------------------------- ------------- 8.5/12.8 MB 1.9 MB/s eta 0:00:03\n",
      "     -------------------------- ------------- 8.6/12.8 MB 1.9 MB/s eta 0:00:03\n",
      "     -------------------------- ------------- 8.6/12.8 MB 1.9 MB/s eta 0:00:03\n",
      "     --------------------------- ------------ 8.7/12.8 MB 1.9 MB/s eta 0:00:03\n",
      "     --------------------------- ------------ 8.8/12.8 MB 1.9 MB/s eta 0:00:03\n",
      "     --------------------------- ------------ 8.9/12.8 MB 1.9 MB/s eta 0:00:03\n",
      "     --------------------------- ------------ 9.0/12.8 MB 1.9 MB/s eta 0:00:02\n",
      "     ---------------------------- ----------- 9.0/12.8 MB 1.9 MB/s eta 0:00:02\n",
      "     ---------------------------- ----------- 9.1/12.8 MB 1.9 MB/s eta 0:00:02\n",
      "     ---------------------------- ----------- 9.2/12.8 MB 1.9 MB/s eta 0:00:02\n",
      "     ----------------------------- ---------- 9.3/12.8 MB 1.9 MB/s eta 0:00:02\n",
      "     ----------------------------- ---------- 9.4/12.8 MB 1.9 MB/s eta 0:00:02\n",
      "     ----------------------------- ---------- 9.6/12.8 MB 1.9 MB/s eta 0:00:02\n",
      "     ------------------------------ --------- 9.6/12.8 MB 1.9 MB/s eta 0:00:02\n",
      "     ------------------------------ --------- 9.8/12.8 MB 1.9 MB/s eta 0:00:02\n",
      "     ------------------------------ --------- 9.9/12.8 MB 1.9 MB/s eta 0:00:02\n",
      "     ------------------------------- -------- 10.1/12.8 MB 1.9 MB/s eta 0:00:02\n",
      "     ------------------------------- -------- 10.2/12.8 MB 1.9 MB/s eta 0:00:02\n",
      "     -------------------------------- ------- 10.3/12.8 MB 2.0 MB/s eta 0:00:02\n",
      "     -------------------------------- ------- 10.3/12.8 MB 2.0 MB/s eta 0:00:02\n",
      "     -------------------------------- ------- 10.4/12.8 MB 2.0 MB/s eta 0:00:02\n",
      "     --------------------------------- ------ 10.6/12.8 MB 2.0 MB/s eta 0:00:02\n",
      "     --------------------------------- ------ 10.8/12.8 MB 2.0 MB/s eta 0:00:01\n",
      "     ---------------------------------- ----- 10.9/12.8 MB 2.0 MB/s eta 0:00:01\n",
      "     ---------------------------------- ----- 11.0/12.8 MB 2.0 MB/s eta 0:00:01\n",
      "     ---------------------------------- ----- 11.2/12.8 MB 2.0 MB/s eta 0:00:01\n",
      "     ----------------------------------- ---- 11.2/12.8 MB 2.0 MB/s eta 0:00:01\n",
      "     ----------------------------------- ---- 11.2/12.8 MB 2.0 MB/s eta 0:00:01\n",
      "     ------------------------------------ --- 11.6/12.8 MB 2.0 MB/s eta 0:00:01\n",
      "     ------------------------------------ --- 11.7/12.8 MB 2.0 MB/s eta 0:00:01\n",
      "     ------------------------------------ --- 11.8/12.8 MB 2.0 MB/s eta 0:00:01\n",
      "     ------------------------------------- -- 11.9/12.8 MB 2.0 MB/s eta 0:00:01\n",
      "     ------------------------------------- -- 11.9/12.8 MB 2.0 MB/s eta 0:00:01\n",
      "     ------------------------------------- -- 12.0/12.8 MB 2.0 MB/s eta 0:00:01\n",
      "     ------------------------------------- -- 12.1/12.8 MB 2.0 MB/s eta 0:00:01\n",
      "     ------------------------------------- -- 12.2/12.8 MB 2.0 MB/s eta 0:00:01\n",
      "     -------------------------------------- - 12.3/12.8 MB 2.0 MB/s eta 0:00:01\n",
      "     -------------------------------------- - 12.4/12.8 MB 2.0 MB/s eta 0:00:01\n",
      "     -------------------------------------- - 12.5/12.8 MB 2.0 MB/s eta 0:00:01\n",
      "     ---------------------------------------  12.6/12.8 MB 2.0 MB/s eta 0:00:01\n",
      "     ---------------------------------------  12.6/12.8 MB 2.0 MB/s eta 0:00:01\n",
      "     ---------------------------------------  12.7/12.8 MB 2.0 MB/s eta 0:00:01\n",
      "     ---------------------------------------  12.8/12.8 MB 2.0 MB/s eta 0:00:01\n",
      "     ---------------------------------------  12.8/12.8 MB 2.0 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 12.8/12.8 MB 1.9 MB/s eta 0:00:00\n",
      "Requirement already satisfied: spacy<3.8.0,>=3.7.2 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from en-core-web-sm==3.7.1) (3.7.2)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.1.8 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.2)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.3.4)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.9.0)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (6.4.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.66.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.31.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\program files\\python312\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.9.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.2)\n",
      "Requirement already satisfied: setuptools in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (69.0.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (23.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in c:\\program files\\python312\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.23.4)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in c:\\program files\\python312\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\program files\\python312\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2023.11.17)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.4.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.7)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.1.3)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "NEWS_API_KEY = os.getenv(\"NEWS_API_KEY\")\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Ingestion (Pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewsAPILoader:\n",
    "\n",
    "    def __init__(self, api_key):\n",
    "        self.api_key = api_key\n",
    "        self.base_url = \"https://newsapi.org/v2/everything\"\n",
    "\n",
    "    def fetch_news(self, query, from_date=None, to_date=None, page_size=10, language=\"en\", sort_by=\"publishedAt\"):\n",
    "        \"\"\"\n",
    "        Fetches news articles from NewsAPI based on the given parameters.\n",
    "        \n",
    "        Parameters:\n",
    "            query (str): Search query string (e.g., 'AI', '\"Elon Musk\"', 'crypto AND bitcoin NOT ethereum').\n",
    "            from_date (str): Start date for news articles (format: YYYY-MM-DD).\n",
    "            to_date (str): End date for news articles (format: YYYY-MM-DD).\n",
    "            page_size (int): Number of articles to retrieve per request (max 100).\n",
    "            language (str): Language of articles (default: 'en').\n",
    "            sort_by (str): Sorting order ('relevancy', 'popularity', 'publishedAt').\n",
    "        \n",
    "        Returns:\n",
    "            list: List of retrieved news articles.\n",
    "        \"\"\"\n",
    "        params = {\n",
    "            \"q\": query,  # Supports Boolean operators like AND, OR, NOT\n",
    "            \"apiKey\": self.api_key,\n",
    "            \"pageSize\": page_size,\n",
    "            \"language\": language,\n",
    "            \"sortBy\": sort_by,\n",
    "        }\n",
    "        \n",
    "        # Handling optional date filters\n",
    "        if from_date:\n",
    "            params[\"from\"] = from_date\n",
    "        if to_date:\n",
    "            params[\"to\"] = to_date\n",
    "\n",
    "        try:\n",
    "            response = requests.get(self.base_url, params=params)\n",
    "            \n",
    "            response.raise_for_status()  # Raising an error for HTTP issues\n",
    "\n",
    "            data = response.json()\n",
    "\n",
    "            if data[\"status\"] == \"ok\":\n",
    "                return data[\"articles\"]\n",
    "            else:\n",
    "                print(f\"Error: {data.get('message', 'Unknown error')}\")\n",
    "                \n",
    "                return None\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"API Request Failed: {e}\")\n",
    "            \n",
    "            return None\n",
    "\n",
    "    def save_to_csv(self, articles, filename=\"news_data.csv\"):\n",
    "        \"\"\"\n",
    "        Saves news articles to a CSV file.\n",
    "\n",
    "        Parameters:\n",
    "            articles (list): List of article dictionaries.\n",
    "            filename (str): Output CSV filename.\n",
    "        \"\"\"\n",
    "\n",
    "        if len(articles) == 0:\n",
    "            print(\"No articles found!\")\n",
    "        \n",
    "            return\n",
    "\n",
    "        # Extracting relevant fields\n",
    "        data = []\n",
    "        \n",
    "        for keyword_based_articles in articles:\n",
    "        \n",
    "            for article in keyword_based_articles:\n",
    "                data.append({\n",
    "                    \"Title\": article.get(\"title\", \"\"),\n",
    "                    \"Content\": article.get(\"content\", \"\"),  # Full content if available\n",
    "                    \"Description\": article.get(\"description\", \"\"),\n",
    "                    \"URL\": article.get(\"url\", \"\"),\n",
    "                    \"Source\": article[\"source\"][\"name\"] if article.get(\"source\") else \"Unknown\",\n",
    "                    \"Published Date\": article.get(\"publishedAt\", \"\"),\n",
    "                    \"Image URL\": article.get(\"urlToImage\", \"\"),\n",
    "                    \"Author\": article.get(\"author\", \"Unknown\"),\n",
    "                })\n",
    "\n",
    "        # Converting to DataFrame and save\n",
    "        df = pd.DataFrame(data)\n",
    "        df.to_csv(filename, index=False)\n",
    "        \n",
    "        print(f\"Data saved to {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataIngestionPipeline:\n",
    "    \n",
    "    def __init__(self, api_key):\n",
    "        self.api_loader = NewsAPILoader(api_key)\n",
    "        # self.file_loader = FileLoader()\n",
    "\n",
    "    def ingest_data(self, keywords=None, file_path=None):\n",
    "        \n",
    "        if len(keywords) > 0:\n",
    "            articles = []\n",
    "\n",
    "            for query in keywords:\n",
    "            \n",
    "                if query:\n",
    "                    from_date = (datetime.now() - timedelta(days=3)).strftime(\"%Y-%m-%d\")\n",
    "                    articles.append(self.api_loader.fetch_news(query=query, from_date=from_date, page_size=5))\n",
    "\n",
    "                # elif file_path:\n",
    "                #     return self.file_loader.load_text(file_path)\n",
    "            \n",
    "            # Saving to CSV\n",
    "            self.api_loader.save_to_csv(articles, f\"../data/news_data.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to ../data/news_data.csv\n"
     ]
    }
   ],
   "source": [
    "data_ingestion_pipeline = DataIngestionPipeline(NEWS_API_KEY)\n",
    "\n",
    "user_query = \"What has been the impact of deepseek on the stock markets worldwide?\"\n",
    "\n",
    "kw_model = KeyBERT()\n",
    "\n",
    "keywords = [item[0] for item in kw_model.extract_keywords(user_query, keyphrase_ngram_range=(1, 2), stop_words=\"english\", top_n=5)]\n",
    "\n",
    "data_ingestion_pipeline.ingest_data(keywords=keywords)\n",
    "# data_ingestion_pipeline.ingest_data(file_path=\"sample.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Content</th>\n",
       "      <th>Description</th>\n",
       "      <th>URL</th>\n",
       "      <th>Source</th>\n",
       "      <th>Published Date</th>\n",
       "      <th>Image URL</th>\n",
       "      <th>Author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alibaba denies it has made $1 billion investme...</td>\n",
       "      <td>On Feb. 7, reports circulated online that Alib...</td>\n",
       "      <td>On Feb. 7, reports circulated online that Alib...</td>\n",
       "      <td>http://technode.com/2025/02/08/alibaba-denies-...</td>\n",
       "      <td>TechNode</td>\n",
       "      <td>2025-02-08T09:38:00Z</td>\n",
       "      <td>https://i0.wp.com/technode.com/wp-content/uplo...</td>\n",
       "      <td>TechNode Feed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Apple Stock Jumps on Artificial Intelligence (...</td>\n",
       "      <td>The news of DeepSeek's launch last week spread...</td>\n",
       "      <td>The news of DeepSeek's launch last week spread...</td>\n",
       "      <td>https://biztoc.com/x/519f28455f9d8f33</td>\n",
       "      <td>Biztoc.com</td>\n",
       "      <td>2025-02-08T09:35:30Z</td>\n",
       "      <td>https://biztoc.com/cdn/808/og.png</td>\n",
       "      <td>aol.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Apple Stock Jumps on Artificial Intelligence (...</td>\n",
       "      <td>If you click 'Accept all', we and our partners...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://consent.yahoo.com/v2/collectConsent?se...</td>\n",
       "      <td>Yahoo Entertainment</td>\n",
       "      <td>2025-02-08T09:05:00Z</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Stock-market concentration rivals 1929 and 199...</td>\n",
       "      <td>How much longer can the Magnificent Seven stoc...</td>\n",
       "      <td>How much longer can the Magnificent Seven stoc...</td>\n",
       "      <td>https://www.businessinsider.com/stock-market-c...</td>\n",
       "      <td>Business Insider</td>\n",
       "      <td>2025-02-08T09:00:01Z</td>\n",
       "      <td>https://i.insider.com/67a667f06630eb10385bf7fe...</td>\n",
       "      <td>wedwards@businessinsider.com (William Edwards)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Cloudastructure, Inc. Class A Common Stock (CS...</td>\n",
       "      <td>We recently compiled a list of the 10 AI News ...</td>\n",
       "      <td>We recently compiled a list of the 10 AI News ...</td>\n",
       "      <td>https://finance.yahoo.com/news/cloudastructure...</td>\n",
       "      <td>Yahoo Entertainment</td>\n",
       "      <td>2025-02-08T07:24:19Z</td>\n",
       "      <td>https://s.yimg.com/ny/api/res/1.2/jES.4QWlDHc9...</td>\n",
       "      <td>Affan Mir</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Title  \\\n",
       "0  Alibaba denies it has made $1 billion investme...   \n",
       "1  Apple Stock Jumps on Artificial Intelligence (...   \n",
       "2  Apple Stock Jumps on Artificial Intelligence (...   \n",
       "3  Stock-market concentration rivals 1929 and 199...   \n",
       "4  Cloudastructure, Inc. Class A Common Stock (CS...   \n",
       "\n",
       "                                             Content  \\\n",
       "0  On Feb. 7, reports circulated online that Alib...   \n",
       "1  The news of DeepSeek's launch last week spread...   \n",
       "2  If you click 'Accept all', we and our partners...   \n",
       "3  How much longer can the Magnificent Seven stoc...   \n",
       "4  We recently compiled a list of the 10 AI News ...   \n",
       "\n",
       "                                         Description  \\\n",
       "0  On Feb. 7, reports circulated online that Alib...   \n",
       "1  The news of DeepSeek's launch last week spread...   \n",
       "2                                                NaN   \n",
       "3  How much longer can the Magnificent Seven stoc...   \n",
       "4  We recently compiled a list of the 10 AI News ...   \n",
       "\n",
       "                                                 URL               Source  \\\n",
       "0  http://technode.com/2025/02/08/alibaba-denies-...             TechNode   \n",
       "1              https://biztoc.com/x/519f28455f9d8f33           Biztoc.com   \n",
       "2  https://consent.yahoo.com/v2/collectConsent?se...  Yahoo Entertainment   \n",
       "3  https://www.businessinsider.com/stock-market-c...     Business Insider   \n",
       "4  https://finance.yahoo.com/news/cloudastructure...  Yahoo Entertainment   \n",
       "\n",
       "         Published Date                                          Image URL  \\\n",
       "0  2025-02-08T09:38:00Z  https://i0.wp.com/technode.com/wp-content/uplo...   \n",
       "1  2025-02-08T09:35:30Z                  https://biztoc.com/cdn/808/og.png   \n",
       "2  2025-02-08T09:05:00Z                                                NaN   \n",
       "3  2025-02-08T09:00:01Z  https://i.insider.com/67a667f06630eb10385bf7fe...   \n",
       "4  2025-02-08T07:24:19Z  https://s.yimg.com/ny/api/res/1.2/jES.4QWlDHc9...   \n",
       "\n",
       "                                           Author  \n",
       "0                                   TechNode Feed  \n",
       "1                                         aol.com  \n",
       "2                                             NaN  \n",
       "3  wedwards@businessinsider.com (William Edwards)  \n",
       "4                                       Affan Mir  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For, testing\n",
    "data = pd.read_csv(\"../data/news_data.csv\")\n",
    "\n",
    "data.head()\n",
    "# data.shape\n",
    "# data[\"Content\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation (Pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataPreparationPipeline:\n",
    "    \n",
    "    def __init__(self, file_paths):\n",
    "        self.file_paths = file_paths\n",
    "        self.nlp = spacy.load(\"en_core_web_sm\")\n",
    "        self.kw_model = KeyBERT()\n",
    "\n",
    "    def clean_text(self, text):\n",
    "\n",
    "        if pd.isna(text):\n",
    "            return \"\"\n",
    "\n",
    "        # text = BeautifulSoup(text, \"html.parser\").get_text()  # If you still need HTML parsing\n",
    "        text = re.sub(r'\\s+', ' ', text)  # Removing extra whitespace\n",
    "        text = re.sub(r'\\[.*?\\]', '', text)  # Removing text in square brackets\n",
    "        text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)  # Removing URLs\n",
    "\n",
    "        # Removing words ending in 3 or more dots, ellipses, or standalone dots\n",
    "        text = re.sub(r'\\b\\w*\\.\\.\\.+\\b', '', text)  \n",
    "        text = re.sub(r'\\b\\w*\\…\\b', '', text)       \n",
    "        text = re.sub(r'\\s*\\.{2,}\\s*', ' ', text)   \n",
    "        text = re.sub(r'\\b\\w*…\\w*\\b', '', text)     \n",
    "\n",
    "        words = text.split()  # Splitting into words\n",
    "        filtered_words = [word for word in words if not re.search(r'\\.\\.\\.|…', word)] \n",
    "        text = ' '.join(filtered_words)  # Reconstructing sentence\n",
    "\n",
    "        import string\n",
    "\n",
    "        # Removing punctuation\n",
    "        text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "        return text.lower().strip()\n",
    "\n",
    "    def remove_duplicates(self, df):\n",
    "        \"\"\"Removes duplicate articles based on name, URL, and publisher.\"\"\"\n",
    "\n",
    "        # Creating a combined key for comparison\n",
    "        df['combined_key'] = df['Title'].astype(str) + df['URL'].astype(str) + df['Author'].astype(str)\n",
    "\n",
    "        # Droping duplicates based on the combined key, keeping the first occurrence\n",
    "        df_deduplicated = df.drop_duplicates(subset='combined_key', keep='first')\n",
    "\n",
    "        # Removing the temporary combined key column\n",
    "        df_deduplicated = df_deduplicated.drop(columns=['combined_key'])\n",
    "\n",
    "        return df_deduplicated\n",
    "    \n",
    "    def detect_language(self, text):\n",
    "        try:\n",
    "            return detect(text)\n",
    "        except:\n",
    "            return \"unknown\"\n",
    "\n",
    "    def extract_entities(self, text):\n",
    "        doc = self.nlp(text)\n",
    "\n",
    "        return {\n",
    "            \"Persons\": [ent.text for ent in doc.ents if ent.label_ == \"PERSON\"],\n",
    "            \"Organizations\": [ent.text for ent in doc.ents if ent.label_ == \"ORG\"],\n",
    "            \"Locations\": [ent.text for ent in doc.ents if ent.label_ in [\"GPE\", \"LOC\"]]\n",
    "        }\n",
    "\n",
    "    def extract_keywords(self, text):\n",
    "        return self.kw_model.extract_keywords(text, keyphrase_ngram_range=(1, 2), stop_words=\"english\", top_n=5)\n",
    "\n",
    "    def chunk_text(self, text, chunk_size=512, overlap=50):\n",
    "        words = text.split()\n",
    "\n",
    "        return [\" \".join(words[i:i + chunk_size]) for i in range(0, len(words), chunk_size - overlap)]\n",
    "\n",
    "    def data_saving(self, df):\n",
    "    \n",
    "        for file_path in self.file_paths:\n",
    "\n",
    "            if os.path.exists(file_path):\n",
    "            \n",
    "                try:\n",
    "                    os.remove(file_path)\n",
    "                except OSError as e:\n",
    "                    print(f\"Error deleting file '{file_path}': {e}\")\n",
    "            \n",
    "            if file_path.endswith(\".json\"):\n",
    "                df.to_json(file_path, orient=\"records\", indent=4)\n",
    "            else:\n",
    "                df.to_csv(file_path, index=False)\n",
    "\n",
    "    def prepare_data(self, df):\n",
    "        df = self.remove_duplicates(df)\n",
    "\n",
    "        print(f\"✅ Deduplication Completed! {len(df)} unique articles remain. Remaining articles: {len(df)}\")\n",
    "        \n",
    "        df[\"Language\"] = df[\"Content\"].apply(self.detect_language)\n",
    "        df = df[df[\"Language\"] == \"en\"]\n",
    "\n",
    "        print(f\"✅ Language Filtering Completed! Remaining articles: {len(df)}\")\n",
    "        \n",
    "        df[\"Entities\"] = df[\"Content\"].apply(self.extract_entities)\n",
    "        df[\"Keywords\"] = df[\"Content\"].apply(self.extract_keywords)\n",
    "\n",
    "        print(f\"✅ Named Entity Recognition (NER) & Topic Extraction Completed! Remaining articles: {len(df)}\")\n",
    "        \n",
    "        df[\"Chunks\"] = df[\"Content\"].apply(self.chunk_text)\n",
    "\n",
    "        print(f\"✅ Chunking Completed! Remaining articles: {len(df)}\")\n",
    "        \n",
    "        df[\"Metadata\"] = df.apply(lambda row: {\n",
    "            \"Source\": row[\"Source\"],\n",
    "            \"Published Date\": row[\"Published Date\"],\n",
    "            \"Keywords\": row[\"Keywords\"],\n",
    "            \"Entities\": row[\"Entities\"]\n",
    "        }, axis=1)\n",
    "\n",
    "        print(f\"✅ Annotation Completed! Remaining articles: {len(df)}\")\n",
    "\n",
    "        df[\"Title\"] = df[\"Title\"].apply(self.clean_text)\n",
    "        df[\"Content\"] = df[\"Content\"].apply(self.clean_text)\n",
    "        df[\"Description\"] = df[\"Description\"].apply(self.clean_text)\n",
    "\n",
    "        print(f\"✅ Text Cleaning Completed! Remaining articles: {len(df)}\")\n",
    "        \n",
    "        self.data_saving(df)\n",
    "\n",
    "        print(f\"✅ Data Preparation Completed! Remaining articles: {len(df)}\")\n",
    "\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_paths = [\n",
    "    \"../data/processed_news_data.json\",\n",
    "    \"../data/processed_news_data.csv\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Deduplication Completed! 17 unique articles remain. Remaining articles: 17\n",
      "✅ Language Filtering Completed! Remaining articles: 17\n",
      "✅ Named Entity Recognition (NER) & Topic Extraction Completed! Remaining articles: 17\n",
      "✅ Chunking Completed! Remaining articles: 17\n",
      "✅ Annotation Completed! Remaining articles: 17\n",
      "✅ Text Cleaning Completed! Remaining articles: 17\n",
      "✅ Data Preparation Completed! Remaining articles: 17\n"
     ]
    }
   ],
   "source": [
    "data_preparation_pipeline = DataPreparationPipeline(file_paths)\n",
    "\n",
    "processed_data = data_preparation_pipeline.prepare_data(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'on feb 7 reports circulated online that alibaba was looking to invest 1 billion in deepseek to take a 10 stake valuing the ai company at 10 billion and potentially forging a strong partnership'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For, testing\n",
    "processed_data.head()\n",
    "\n",
    "processed_data[\"Content\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Embedding and Indexing (Pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Content</th>\n",
       "      <th>Description</th>\n",
       "      <th>URL</th>\n",
       "      <th>Source</th>\n",
       "      <th>Published Date</th>\n",
       "      <th>Image URL</th>\n",
       "      <th>Author</th>\n",
       "      <th>Language</th>\n",
       "      <th>Entities</th>\n",
       "      <th>Keywords</th>\n",
       "      <th>Chunks</th>\n",
       "      <th>Metadata</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>alibaba denies it has made 1 billion investmen...</td>\n",
       "      <td>on feb 7 reports circulated online that alibab...</td>\n",
       "      <td>on feb 7 reports circulated online that alibab...</td>\n",
       "      <td>http://technode.com/2025/02/08/alibaba-denies-...</td>\n",
       "      <td>TechNode</td>\n",
       "      <td>2025-02-08T09:38:00Z</td>\n",
       "      <td>https://i0.wp.com/technode.com/wp-content/uplo...</td>\n",
       "      <td>TechNode Feed</td>\n",
       "      <td>en</td>\n",
       "      <td>{'Persons': [], 'Organizations': ['AI'], 'Loca...</td>\n",
       "      <td>[[billion deepseek, 0.6047], [deepseek, 0.5423...</td>\n",
       "      <td>[On Feb. 7, reports circulated online that Ali...</td>\n",
       "      <td>{'Source': 'TechNode', 'Published Date': '2025...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>apple stock jumps on artificial intelligence a...</td>\n",
       "      <td>the news of deepseeks launch last week spread ...</td>\n",
       "      <td>the news of deepseeks launch last week spread ...</td>\n",
       "      <td>https://biztoc.com/x/519f28455f9d8f33</td>\n",
       "      <td>Biztoc.com</td>\n",
       "      <td>2025-02-08T09:35:30Z</td>\n",
       "      <td>https://biztoc.com/cdn/808/og.png</td>\n",
       "      <td>aol.com</td>\n",
       "      <td>en</td>\n",
       "      <td>{'Persons': [], 'Organizations': ['DeepSeek', ...</td>\n",
       "      <td>[[deepseek launch, 0.6475000000000001], [deeps...</td>\n",
       "      <td>[The news of DeepSeek's launch last week sprea...</td>\n",
       "      <td>{'Source': 'Biztoc.com', 'Published Date': '20...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>apple stock jumps on artificial intelligence a...</td>\n",
       "      <td>if you click accept all we and our partners in...</td>\n",
       "      <td></td>\n",
       "      <td>https://consent.yahoo.com/v2/collectConsent?se...</td>\n",
       "      <td>Yahoo Entertainment</td>\n",
       "      <td>2025-02-08T09:05:00Z</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>en</td>\n",
       "      <td>{'Persons': ['Consent Framework'], 'Organizati...</td>\n",
       "      <td>[[access information, 0.5177], [iab transparen...</td>\n",
       "      <td>[If you click 'Accept all', we and our partner...</td>\n",
       "      <td>{'Source': 'Yahoo Entertainment', 'Published D...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>stockmarket concentration rivals 1929 and 1999...</td>\n",
       "      <td>how much longer can the magnificent seven stoc...</td>\n",
       "      <td>how much longer can the magnificent seven stoc...</td>\n",
       "      <td>https://www.businessinsider.com/stock-market-c...</td>\n",
       "      <td>Business Insider</td>\n",
       "      <td>2025-02-08T09:00:01Z</td>\n",
       "      <td>https://i.insider.com/67a667f06630eb10385bf7fe...</td>\n",
       "      <td>wedwards@businessinsider.com (William Edwards)</td>\n",
       "      <td>en</td>\n",
       "      <td>{'Persons': ['Glenmede'], 'Organizations': [],...</td>\n",
       "      <td>[[seven stocks, 0.6125], [stocks continue, 0.5...</td>\n",
       "      <td>[How much longer can the Magnificent Seven sto...</td>\n",
       "      <td>{'Source': 'Business Insider', 'Published Date...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cloudastructure inc class a common stock csai ...</td>\n",
       "      <td>we recently compiled a list of the 10 ai news ...</td>\n",
       "      <td>we recently compiled a list of the 10 ai news ...</td>\n",
       "      <td>https://finance.yahoo.com/news/cloudastructure...</td>\n",
       "      <td>Yahoo Entertainment</td>\n",
       "      <td>2025-02-08T07:24:19Z</td>\n",
       "      <td>https://s.yimg.com/ny/api/res/1.2/jES.4QWlDHc9...</td>\n",
       "      <td>Affan Mir</td>\n",
       "      <td>en</td>\n",
       "      <td>{'Persons': [], 'Organizations': ['Cloudastruc...</td>\n",
       "      <td>[[nasdaq csai, 0.5572], [stock nasdaq, 0.5526]...</td>\n",
       "      <td>[We recently compiled a list of the 10 AI News...</td>\n",
       "      <td>{'Source': 'Yahoo Entertainment', 'Published D...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Title  \\\n",
       "0  alibaba denies it has made 1 billion investmen...   \n",
       "1  apple stock jumps on artificial intelligence a...   \n",
       "2  apple stock jumps on artificial intelligence a...   \n",
       "3  stockmarket concentration rivals 1929 and 1999...   \n",
       "4  cloudastructure inc class a common stock csai ...   \n",
       "\n",
       "                                             Content  \\\n",
       "0  on feb 7 reports circulated online that alibab...   \n",
       "1  the news of deepseeks launch last week spread ...   \n",
       "2  if you click accept all we and our partners in...   \n",
       "3  how much longer can the magnificent seven stoc...   \n",
       "4  we recently compiled a list of the 10 ai news ...   \n",
       "\n",
       "                                         Description  \\\n",
       "0  on feb 7 reports circulated online that alibab...   \n",
       "1  the news of deepseeks launch last week spread ...   \n",
       "2                                                      \n",
       "3  how much longer can the magnificent seven stoc...   \n",
       "4  we recently compiled a list of the 10 ai news ...   \n",
       "\n",
       "                                                 URL               Source  \\\n",
       "0  http://technode.com/2025/02/08/alibaba-denies-...             TechNode   \n",
       "1              https://biztoc.com/x/519f28455f9d8f33           Biztoc.com   \n",
       "2  https://consent.yahoo.com/v2/collectConsent?se...  Yahoo Entertainment   \n",
       "3  https://www.businessinsider.com/stock-market-c...     Business Insider   \n",
       "4  https://finance.yahoo.com/news/cloudastructure...  Yahoo Entertainment   \n",
       "\n",
       "         Published Date                                          Image URL  \\\n",
       "0  2025-02-08T09:38:00Z  https://i0.wp.com/technode.com/wp-content/uplo...   \n",
       "1  2025-02-08T09:35:30Z                  https://biztoc.com/cdn/808/og.png   \n",
       "2  2025-02-08T09:05:00Z                                               None   \n",
       "3  2025-02-08T09:00:01Z  https://i.insider.com/67a667f06630eb10385bf7fe...   \n",
       "4  2025-02-08T07:24:19Z  https://s.yimg.com/ny/api/res/1.2/jES.4QWlDHc9...   \n",
       "\n",
       "                                           Author Language  \\\n",
       "0                                   TechNode Feed       en   \n",
       "1                                         aol.com       en   \n",
       "2                                            None       en   \n",
       "3  wedwards@businessinsider.com (William Edwards)       en   \n",
       "4                                       Affan Mir       en   \n",
       "\n",
       "                                            Entities  \\\n",
       "0  {'Persons': [], 'Organizations': ['AI'], 'Loca...   \n",
       "1  {'Persons': [], 'Organizations': ['DeepSeek', ...   \n",
       "2  {'Persons': ['Consent Framework'], 'Organizati...   \n",
       "3  {'Persons': ['Glenmede'], 'Organizations': [],...   \n",
       "4  {'Persons': [], 'Organizations': ['Cloudastruc...   \n",
       "\n",
       "                                            Keywords  \\\n",
       "0  [[billion deepseek, 0.6047], [deepseek, 0.5423...   \n",
       "1  [[deepseek launch, 0.6475000000000001], [deeps...   \n",
       "2  [[access information, 0.5177], [iab transparen...   \n",
       "3  [[seven stocks, 0.6125], [stocks continue, 0.5...   \n",
       "4  [[nasdaq csai, 0.5572], [stock nasdaq, 0.5526]...   \n",
       "\n",
       "                                              Chunks  \\\n",
       "0  [On Feb. 7, reports circulated online that Ali...   \n",
       "1  [The news of DeepSeek's launch last week sprea...   \n",
       "2  [If you click 'Accept all', we and our partner...   \n",
       "3  [How much longer can the Magnificent Seven sto...   \n",
       "4  [We recently compiled a list of the 10 AI News...   \n",
       "\n",
       "                                            Metadata  \n",
       "0  {'Source': 'TechNode', 'Published Date': '2025...  \n",
       "1  {'Source': 'Biztoc.com', 'Published Date': '20...  \n",
       "2  {'Source': 'Yahoo Entertainment', 'Published D...  \n",
       "3  {'Source': 'Business Insider', 'Published Date...  \n",
       "4  {'Source': 'Yahoo Entertainment', 'Published D...  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_json(\"../data/processed_news_data.json\")\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility function(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checking_for_file_existence(file_path):\n",
    "    \n",
    "    if os.path.exists(file_path):\n",
    "            \n",
    "        try:\n",
    "            os.remove(file_path)\n",
    "        except OSError as e:\n",
    "            print(f\"Error deleting file '{file_path}': {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingGenerator:\n",
    "    \n",
    "    def __init__(self, model_name=\"all-MiniLM-L6-v2\"):\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "\n",
    "    def generate_embedding(self, text):\n",
    "        return self.model.encode(text, convert_to_numpy=True)\n",
    "\n",
    "class VectorDatabase:\n",
    "    \n",
    "    def __init__(self, embedding_dim=384, index_file=\"../data/faiss_index.bin\", metadata_file=\"../data/metadata.json\"):\n",
    "        self.index = faiss.IndexFlatL2(embedding_dim)  # L2 (Euclidean) distance index\n",
    "        self.metadata = []\n",
    "        \n",
    "        checking_for_file_existence(index_file)\n",
    "        \n",
    "        self.index_file = index_file\n",
    "        \n",
    "        checking_for_file_existence(metadata_file)\n",
    "        \n",
    "        self.metadata_file = metadata_file\n",
    "\n",
    "    def add_embedding(self, embedding, metadata):\n",
    "        \"\"\"Adds a new embedding and its metadata.\"\"\"\n",
    "\n",
    "        self.index.add(np.array([embedding], dtype=np.float32))\n",
    "        self.metadata.append(metadata)\n",
    "\n",
    "    def search(self, query_embedding, top_k=5):\n",
    "        \"\"\"Performs similarity search and retrieves top_k results.\"\"\"\n",
    "        \n",
    "        distances, indices = self.index.search(np.array([query_embedding], dtype=np.float32), top_k)\n",
    "        \n",
    "        return [(self.metadata[i], distances[0][j]) for j, i in enumerate(indices[0]) if i < len(self.metadata)]\n",
    "\n",
    "    def save_index(self):\n",
    "        \"\"\"Saves FAISS index and metadata to disk.\"\"\"\n",
    "        \n",
    "        faiss.write_index(self.index, self.index_file)\n",
    "        \n",
    "        import json\n",
    "\n",
    "        with open(self.metadata_file, \"w\") as f:\n",
    "            json.dump(self.metadata, f, indent=4)\n",
    "        \n",
    "        print(f\"✅ FAISS index saved to {self.index_file}\")\n",
    "        print(f\"✅ Metadata saved to {self.metadata_file}\")\n",
    "\n",
    "    def load_index(self):\n",
    "        \"\"\"Loads FAISS index and metadata from disk.\"\"\"\n",
    "        \n",
    "        if os.path.exists(self.index_file):\n",
    "            self.index = faiss.read_index(self.index_file)\n",
    "            print(f\"✅ FAISS index loaded from {self.index_file}\")\n",
    "        else:\n",
    "            print(f\"⚠️ No FAISS index found at {self.index_file}\")\n",
    "\n",
    "        if os.path.exists(self.metadata_file):\n",
    "            import json\n",
    "            \n",
    "            with open(self.metadata_file, \"r\") as f:\n",
    "                self.metadata = json.load(f)\n",
    "            print(f\"✅ Metadata loaded from {self.metadata_file}\")\n",
    "        else:\n",
    "            print(f\"⚠️ No metadata found at {self.metadata_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataEmbeddingPipeline:\n",
    "    \n",
    "    def __init__(self, model_name=\"all-MiniLM-L6-v2\", embedding_dim=384):\n",
    "        self.embedder = EmbeddingGenerator(model_name)\n",
    "        self.vector_db = VectorDatabase(embedding_dim)\n",
    "        self.vector_db.load_index()  # Loading index on initialization\n",
    "\n",
    "    def process_and_store(self, articles):\n",
    "        \"\"\"Processes articles, extracts relevant information, generates embeddings, and stores them.\"\"\"\n",
    "        \n",
    "        for article in articles:\n",
    "            # Constructing a text input with relevant fields\n",
    "            text = f\"{article.get('Title', '')} {article.get('Content', '')} {article.get('Description', '')}\"\n",
    "            \n",
    "            # Extracting keywords and entities\n",
    "            keywords = [kw[0] for kw in article.get(\"Keywords\", [])]\n",
    "            persons = article.get(\"Entities\", {}).get(\"Persons\", [])\n",
    "            organizations = article.get(\"Entities\", {}).get(\"Organizations\", [])\n",
    "            locations = article.get(\"Entities\", {}).get(\"Locations\", [])\n",
    "\n",
    "            # Generating the embedding\n",
    "            embedding = self.embedder.generate_embedding(text)\n",
    "\n",
    "            # Storing data\n",
    "            data = {\n",
    "                \"Source\": article.get(\"Source\", \"\"),\n",
    "                \"Published Date\": article.get(\"Published Date\", \"\"),\n",
    "                \"Keywords\": keywords,\n",
    "                \"Entities\": {\n",
    "                    \"Persons\": persons,\n",
    "                    \"Organizations\": organizations,\n",
    "                    \"Locations\": locations\n",
    "                },\n",
    "                \"URL\": article.get(\"URL\", \"\"),\n",
    "                \"Title\": article.get('Title', ''),\n",
    "                \"Content\": article.get(\"Content\", ''),\n",
    "                \"Description\": article.get(\"Description\", '')\n",
    "            }\n",
    "            self.vector_db.add_embedding(embedding, data)\n",
    "\n",
    "        self.vector_db.save_index()  # Saving index after processing\n",
    "\n",
    "    def retrieve_similar_articles(self, query_text, top_k=5):\n",
    "        \"\"\"Retrieves similar articles based on semantic similarity.\"\"\"\n",
    "        \n",
    "        query_embedding = self.embedder.generate_embedding(query_text)\n",
    "        \n",
    "        return self.vector_db.search(query_embedding, top_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ No FAISS index found at ../data/faiss_index.bin\n",
      "⚠️ No metadata found at ../data/metadata.json\n",
      "✅ FAISS index saved to ../data/faiss_index.bin\n",
      "✅ Metadata saved to ../data/metadata.json\n",
      "\n",
      "🔍 Search Results:\n",
      "📰 Source: YLE News\n",
      "🌍 Published: 2025-02-08T09:59:57Z\n",
      "🔗 URL: https://yle.fi/a/74-20142384\n",
      "🎯 Similarity Score: 0.9385\n",
      "\n",
      "📰 Source: Fast Company\n",
      "🌍 Published: 2025-02-08T10:00:00Z\n",
      "🔗 URL: https://www.fastcompany.com/91273725/deepseek-benefit-apple-intelligence-china-ai-race-openai-google-microsoft-meta\n",
      "🎯 Similarity Score: 1.0076\n",
      "\n",
      "📰 Source: Yahoo Entertainment\n",
      "🌍 Published: 2025-02-08T09:05:00Z\n",
      "🔗 URL: https://consent.yahoo.com/v2/collectConsent?sessionId=1_cc-session_713cb470-d013-43f6-94ee-23b0db69939c\n",
      "🎯 Similarity Score: 1.4240\n",
      "\n",
      "📰 Source: Yahoo Entertainment\n",
      "🌍 Published: 2025-02-08T07:24:19Z\n",
      "🔗 URL: https://finance.yahoo.com/news/cloudastructure-inc-class-common-stock-072419300.html\n",
      "🎯 Similarity Score: 1.4261\n",
      "\n",
      "📰 Source: Biztoc.com\n",
      "🌍 Published: 2025-02-08T09:35:30Z\n",
      "🔗 URL: https://biztoc.com/x/519f28455f9d8f33\n",
      "🎯 Similarity Score: 1.4549\n",
      "\n"
     ]
    }
   ],
   "source": [
    "embedding_pipeline = DataEmbeddingPipeline()\n",
    "\n",
    "# Loading sample articles from JSON file\n",
    "import json\n",
    "\n",
    "with open(\"../data/processed_news_data.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    articles = json.load(f)\n",
    "\n",
    "embedding_pipeline.process_and_store(articles)  # Storing embeddings\n",
    "\n",
    "query = \"AI revolution in China\"\n",
    "\n",
    "results = embedding_pipeline.retrieve_similar_articles(query, top_k=5)\n",
    "\n",
    "retrieved_articles = []\n",
    "\n",
    "# For, testing\n",
    "print(\"\\n🔍 Search Results:\")\n",
    "for data, distance in results:\n",
    "    retrieved_articles.append((data, distance))\n",
    "    \n",
    "    # print(metadata) # For, testing\n",
    "\n",
    "    print(f\"📰 Source: {data['Source']}\")\n",
    "    print(f\"🌍 Published: {data['Published Date']}\")\n",
    "    print(f\"🔗 URL: {data['URL']}\")\n",
    "    print(f\"🎯 Similarity Score: {distance:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** Already done within the `Data Ingestion` and `Data Embeddings and Indexing Pipelines`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summarization (Pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SummarizationPipeline:\n",
    "\n",
    "    def __init__(self, model_name=\"gpt-4o\", api_key=None):\n",
    "        self.api_key = api_key or os.getenv(\"OPENAI_API_KEY\")\n",
    "    \n",
    "        if not self.api_key:\n",
    "            raise ValueError(\"❌ OpenAI API key is required!\")\n",
    "\n",
    "        self.llm = ChatOpenAI(model_name=model_name, openai_api_key=self.api_key)\n",
    "\n",
    "        # Defining a prompt template for summarization\n",
    "        self.prompt_template = PromptTemplate.from_template(\n",
    "            '''You are a professional journalist summarizing news articles. \n",
    "            - Summarize each article (provided in the form of a list; the list contains the title, content and description of each of the articles) in **3-4 sentences**, keeping key details.\n",
    "            - At the end, provide a **brief summary** of all the articles.\n",
    "            - Draw **insights into future trends** based on the summarized content.\n",
    "            \n",
    "            ### **Example Response Format**:\n",
    "            \n",
    "            **Article 1: (Title of it)**\n",
    "            📌 summary_1\n",
    "            \n",
    "            **Article 2: (Title of it)**\n",
    "            📌 summary_2\n",
    "            \n",
    "            ...\n",
    "            \n",
    "            ### **Combined Summary & Future Trends**\n",
    "            📊 combined_summary\n",
    "            📈 future_trends\n",
    "            \n",
    "            Now, summarize the following articles accordingly:\n",
    "            \n",
    "            {article_text}\n",
    "            '''\n",
    "        )\n",
    "\n",
    "    def summarize_article(self, article_text):\n",
    "        \"\"\"Generate a summary for a single news article.\"\"\"\n",
    "        prompt = self.prompt_template.format(article_text=article_text)\n",
    "        response = self.llm([HumanMessage(content=prompt)])\n",
    "        \n",
    "        return response.content.strip()\n",
    "\n",
    "    def summarize_retrieved_articles(self, retrieved_articles):\n",
    "        \"\"\"Generate summaries for multiple retrieved articles.\"\"\"\n",
    "        summaries = {}\n",
    "\n",
    "        texts = []\n",
    "\n",
    "        for data, score in retrieved_articles: \n",
    "            texts.append(f\"Title: {data.get('Title', '')}, Content:{data.get('Content', '')} and Description:{data.get('Description', '')}\")  \n",
    "\n",
    "            # print(text) # For, testing\n",
    "        \n",
    "        return self.summarize_article(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📜 **Summarized Articles:**\n",
      "**Article 1: AI Expert Explores Finland’s Role in Global Tech Race**  \n",
      "📌 Finland is positioning itself in the global AI competition by leveraging innovation that caters to its unique language and culture. Peter Sarlin, founder of Silo AI, emphasizes the importance of sovereignty, innovation, and education in enhancing Finland's competitiveness on the global stage. Finland's approach highlights the strategic use of cultural and linguistic assets to carve out a niche in the AI sector.\n",
      "\n",
      "**Article 2: How Apple Could Work with DeepSeek to Pull Ahead in the AI Race**  \n",
      "📌 The emergence of Chinese startup DeepSeek has sparked discussions about its impact on the AI strategies of major US companies like OpenAI, Microsoft, Google, and Meta. DeepSeek's accomplishments could potentially influence Apple’s strategy to collaborate or compete in the AI sector. There is speculation about how Apple might leverage partnerships to maintain its edge in the rapidly evolving AI landscape.\n",
      "\n",
      "**Article 3: Apple Stock Jumps on Artificial Intelligence Driving iPhone Sales: Here's Why It's Not Getting Crushed by the DeepSeek Launch**  \n",
      "📌 Despite the disruptive potential of DeepSeek's recent launch, which negatively impacted Nvidia's stock, Apple has maintained stability. The integration of AI into Apple's product ecosystem, notably driving iPhone sales, is credited with shielding it from the market upheaval faced by other AI-related stocks. Apple's strategic focus on AI innovations within its devices continues to sustain its stock performance.\n",
      "\n",
      "**Article 4: Cloudastructure Inc Class A Common Stock (CSAI) Expands Sales Team to Meet Growing Demand for AI Security Solutions**  \n",
      "📌 Cloudastructure Inc is responding to the increasing demand for AI security solutions by expanding its sales team. This move is part of a broader trend in the AI market, where security and infrastructure companies are scaling operations to meet the needs of a rapidly growing sector. The company's strategic expansion reflects optimism about the future of AI-driven security solutions.\n",
      "\n",
      "**Article 5: Apple Stock Jumps on Artificial Intelligence Driving iPhone Sales: Here's Why It's Not Getting Crushed by the DeepSeek Launch**  \n",
      "📌 Apple's stock has remained resilient despite DeepSeek's disruptive market entry, which significantly impacted stocks like Nvidia. The continued strength of Apple's stock is attributed to the successful integration of AI technologies into its products, particularly iPhones. This resilience demonstrates Apple's ability to adapt and thrive amidst competitive pressures in the AI sector.\n",
      "\n",
      "### **Combined Summary & Future Trends**  \n",
      "📊 The articles collectively highlight the dynamic and competitive landscape of the AI industry, showcasing Finland's unique cultural approach, Apple's strategic resilience, and the impact of new entrants like DeepSeek. Cloudastructure's expansion points to a growing demand for AI security solutions, indicative of broader industry trends.\n",
      "\n",
      "📈 The future of AI will likely see increased collaboration between tech giants and innovative startups, as companies like Apple might seek partnerships to enhance their AI capabilities. The demand for AI security solutions is expected to rise, prompting companies to expand their operations and refine their offerings. Additionally, leveraging cultural uniqueness, as Finland is doing, could become a more common strategy for countries and companies seeking to establish a competitive edge in the global AI market.\n"
     ]
    }
   ],
   "source": [
    "summarizer = SummarizationPipeline(api_key=OPENAI_API_KEY)  \n",
    "\n",
    "summaries = summarizer.summarize_retrieved_articles(retrieved_articles)\n",
    "\n",
    "from titlecase import titlecase\n",
    "  \n",
    "print(\"\\n📜 **Summarized Articles:**\")\n",
    "print(summaries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Propagation delay:\n",
    "\\(d_{prop} = \\frac{\\text{distance}}{\\text{speed}} = \\frac{2 \\times 10^7}{2.5 \\times 10^8} = 0.08 \\text{ sec}\\)\n",
    "\n",
    "Bandwidth-delay product:\n",
    "\\(R \\times d_{prop} = 5 \\times 10^6 \\times 0.08 = 400,000 \\text{ bits}\\)\n",
    "\n",
    "Bit width:\n",
    "\\(\\text{Bit width} = \\frac{\\text{speed}}{\\text{rate}} = \\frac{2.5 \\times 10^8}{5 \\times 10^6} = 50 \\text{ meters}\\)\n",
    "\n",
    "Alternatively,\n",
    "\\(\\text{Bit width} = \\frac{s}{R}\\)\n",
    "\n",
    "Transmission delay:\n",
    "\\(\\text{Transmission delay} = \\frac{L}{R} = \\frac{800,000}{5 \\times 10^6} = 0.16 \\text{ sec}\\)\n",
    "\n",
    "Total time:\n",
    "\\(0.16 \\text{ sec}\\)\n",
    "\n",
    "End-to-end delay:\n",
    "\\(\\text{End-to-end delay} = \\frac{175}{100} = 1.75 \\text{ hours}\\)\n",
    "\n",
    "For a 500-byte packet:\n",
    "\\(T_{trans} = \\frac{500 \\times 8}{R}\\)\n",
    "\n",
    "For a 125-byte packet:\n",
    "\\(T_{trans} = \\frac{125 \\times 8}{R}\\)\n",
    "\n",
    "Propagation delay formula:\n",
    "\\(T_{prop} = \\frac{d}{s}\\)\n",
    "\n",
    "For Link 1:\n",
    "\\(T_{prop_{A \\to B}}\\)\n",
    "\n",
    "For Link 2:\n",
    "\\(T_{prop_{B \\to C}}\\)\n",
    "\n",
    "Total time:\n",
    "\\(T_{total} = T_{trans_{A \\to B}} + T_{prop_{A \\to B}} + T_{trans_{B \\to C}} + T_{prop_{B \\to C}}\\)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
